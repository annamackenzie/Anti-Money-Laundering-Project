{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load confidential credential data\n",
    "import aml_private\n",
    "\n",
    "apikey = aml_private.api_key\n",
    "url_template = 'https://api.companieshouse.gov.uk{}'\n",
    "request_count = {'count':0, 'reset_time':datetime.now()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Request from api whilst counting total requests in 5 minute period in line with CH rate limit\n",
    "\n",
    "def request_api(url, apikey, counter):\n",
    "    # Remaining time of 5 min period since last reset else 0 if more than 5 mins since last reset \n",
    "    time_remaining = max(5*60-(datetime.now()-counter['reset_time']).total_seconds(), 0)\n",
    "    \n",
    "    if time_remaining==0:\n",
    "        # Reset counter and make request\n",
    "        counter['count']=1\n",
    "        counter['reset_time']=datetime.now()\n",
    "    \n",
    "    elif counter['count']<500:\n",
    "        # Increment counter and make request\n",
    "        counter['count']+=1\n",
    "        output = requests.get(url, auth=(apikey, '')).json()\n",
    "    \n",
    "    else:\n",
    "        # Wait until end of period then reset counter and make request\n",
    "        time.sleep(time_remaining)\n",
    "        counter['count']=1\n",
    "        counter['reset_time']=datetime.now()\n",
    "        output = requests.get(url, auth=(apikey, '')).json()\n",
    "        print(counter)\n",
    "    \n",
    "    # Update counter and output request\n",
    "    return output, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scrape information from company page\n",
    "\n",
    "def fetch_comp_info(link, url_template, apikey, counter):\n",
    "    # Formulate url and make request\n",
    "    url = url_template.format(link)\n",
    "    r, counter = request_api(url, apikey, counter) \n",
    "    \n",
    "    # Collect basic company information\n",
    "    name = r['company_name']\n",
    "    comp_number = r['company_number']\n",
    "    kind = r['type']\n",
    "    creation_date = r['date_of_creation']\n",
    "    \n",
    "    \n",
    "    # Check for various address elements\n",
    "    try:\n",
    "        address = r['registered_office_address']['address_line_1']\n",
    "    except:\n",
    "        address = ''\n",
    "    try:\n",
    "        address += ' ' + r['registered_office_address']['address_line_2']\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        address += ' ' + r['registered_office_address']['locality']\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        address += ' ' + r['registered_office_address']['country']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Check if company has been dissolved\n",
    "    try:\n",
    "        dissolved_date = r['date_of_cessation']\n",
    "    except:\n",
    "        dissolved_date = ''\n",
    "    \n",
    "    return name, comp_number, address, kind, creation_date, dissolved_date, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scrape officer information from company's officers page\n",
    "\n",
    "def fetch_officers(comp_number, url_template, apikey, counter):\n",
    "    # Formulate url from company number and make request\n",
    "    url = url_template.format('/company/{}/officers'.format(comp_number))\n",
    "    r, counter = request_api(url, apikey, counter)\n",
    "    \n",
    "    # Instantiate lists for each information category\n",
    "    officer_names = []\n",
    "    officer_roles = []\n",
    "    officer_types = []\n",
    "    officer_statuses = []\n",
    "    officer_countries = []\n",
    "    officer_addresses = []\n",
    "    officer_comp_numbers = []\n",
    "\n",
    "    # Check for active officers\n",
    "    try:\n",
    "        num_active = r['active_count']\n",
    "    except KeyError:\n",
    "        num_active = 0\n",
    "    \n",
    "    # Check for inactive officers\n",
    "    try:\n",
    "        num_inactive = r['inactive_count']\n",
    "    except KeyError:\n",
    "        num_inactive = 0\n",
    "    \n",
    "    # Check for resigned officers\n",
    "    try:\n",
    "        num_resigned = r['resigned_count']\n",
    "    except KeyError:\n",
    "        num_resigned = 0\n",
    "    \n",
    "    # For companies with at least one officer\n",
    "    if r != {}:\n",
    "        # Iterate through each officer\n",
    "        for i in r['items']:\n",
    "            \n",
    "            # Look for name\n",
    "            try:\n",
    "                officer_name = i['name']\n",
    "            except:\n",
    "                officer_name = ''\n",
    "                \n",
    "            # Look for role\n",
    "            try:\n",
    "                officer_role = i['officer_role']\n",
    "            except:\n",
    "                officer_role = ''\n",
    "\n",
    "            # Set status as resigned if resignation date present\n",
    "            try:\n",
    "                i['resigned_on']\n",
    "                officer_status = 'resigned'\n",
    "            except:\n",
    "                officer_status = 'active'\n",
    "                \n",
    "            # Check for country\n",
    "            try:\n",
    "                officer_country = i['address']['country']\n",
    "            except:\n",
    "                officer_country = ''\n",
    "\n",
    "            # Look for date of birth to assign type as person\n",
    "            try:\n",
    "                i['date_of_birth']\n",
    "                officer_type = 'person'\n",
    "            except:\n",
    "                officer_type = 'company'\n",
    "             \n",
    "            # Look for address\n",
    "            try:\n",
    "                officer_address = i['address']['address_line_1']\n",
    "            except:\n",
    "                officer_address = ''\n",
    "            try:\n",
    "                officer_address += ' ' + i['address']['address_line_2']\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                officer_address += ' ' + i['address']['locality']\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                officer_address += ' ' + i['address']['country']\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Add details for current officer to list\n",
    "            officer_names.append(officer_name)\n",
    "            officer_roles.append(officer_role)\n",
    "            officer_types.append(officer_type)\n",
    "            officer_statuses.append(officer_status)\n",
    "            officer_countries.append(officer_country)\n",
    "            officer_addresses.append(officer_address)\n",
    "            officer_comp_numbers.append(comp_number)\n",
    "    \n",
    "    return num_active, num_inactive, num_resigned, officer_names, officer_roles, officer_types,\\\n",
    "    officer_statuses, officer_countries, officer_addresses, officer_comp_numbers, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scrape PSC information from company's PSC/RLE page\n",
    "\n",
    "def fetch_psc(comp_number, url_template, apikey, counter):\n",
    "    # Formulate url from company number and make request\n",
    "    url = url_template.format('/company/{}/persons-with-significant-control'.format(comp_number))\n",
    "    r, counter = request_api(url, apikey, counter)\n",
    "    \n",
    "    # Instantiate lists of PSC details\n",
    "    psc_types = []\n",
    "    psc_statuses = []\n",
    "    psc_countries = []\n",
    "    psc_comp_numbers = []\n",
    "    \n",
    "    try:\n",
    "        # Check for number of active and number of ceased PSCs\n",
    "        num_active = r['active_count']\n",
    "        num_ceased = r['ceased_count']\n",
    "        \n",
    "    except:\n",
    "        num_active = 0\n",
    "        num_ceased = 0\n",
    "        \n",
    "    # For companies with at least one PSC\n",
    "    if r!={}:\n",
    "        # Iterate through PSCs\n",
    "        for i in r['items']:\n",
    "            \n",
    "            # Check for ceased date to define status\n",
    "            try:\n",
    "                i['ceased_on']\n",
    "                psc_status = 'ceased'\n",
    "            except:\n",
    "                psc_status = 'active'\n",
    "            \n",
    "            # Check for date of birth to assign type as person\n",
    "            try:\n",
    "                i['date_of_birth']\n",
    "                psc_type = 'person'\n",
    "            except:\n",
    "                try:\n",
    "                    psc_type = i['identification']['legal_form']\n",
    "                except:\n",
    "                    psc_type = ''\n",
    "            \n",
    "            # Check for country\n",
    "            try:\n",
    "                psc_country = i['address']['country']\n",
    "            except:\n",
    "                psc_country = ''\n",
    "\n",
    "            # Update list with details of current PSC\n",
    "            psc_types.append(psc_type)\n",
    "            psc_statuses.append(psc_status)\n",
    "            psc_countries.append(psc_country)\n",
    "            psc_comp_numbers.append(comp_number)\n",
    "\n",
    "        \n",
    "    return num_active, num_ceased, psc_types, psc_statuses, psc_countries, psc_comp_numbers, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scrape filing information from company's filing history page\n",
    "\n",
    "def fetch_filings(comp_number, url_template, apikey, counter):\n",
    "    # Formulate url and make request\n",
    "    url = url_template.format('/company/{}/filing-history'.format(comp_number))\n",
    "    r, counter = request_api(url, apikey, counter)\n",
    "    \n",
    "    # Instatiate lists for filings information\n",
    "    comp_numbers = []\n",
    "    filing_dates = []\n",
    "    filing_descriptions = []\n",
    "    name_change = []\n",
    "    \n",
    "    # Define starting point for iterating through multiple pages\n",
    "    start_index = 0\n",
    "    \n",
    "    # Check if current page is last page\n",
    "    while int(r['start_index']) + int(r['items_per_page']) < r['total_count']:\n",
    "        # Check items per page, if final page then count of actual items on page \n",
    "        r_items = min(r['items_per_page'], r['total_count']-r['start_index'])\n",
    "        \n",
    "        # Iterate through each result\n",
    "        for i in range(r_items):\n",
    "            # Add company number as identifier\n",
    "            comp_numbers.append(comp_number)\n",
    "            \n",
    "            # Collect date and description of each filing\n",
    "            filing_dates.append(r['items'][i]['date'])\n",
    "            filing_descriptions.append(r['items'][i]['description'])\n",
    "            \n",
    "            # Check if filing is company name change\n",
    "            try:\n",
    "                # If a resolution check if type name change\n",
    "                if r['items'][i]['resolutions'][0]['category'] == 'change-of-name':\n",
    "                    name_change.append(1)\n",
    "                else:\n",
    "                    name_change.append(0)\n",
    "            \n",
    "            # If not a resolution, mark as not a change of name\n",
    "            except:\n",
    "                name_change.append(0)\n",
    "        \n",
    "        # Increment to next page of results, reformat url and make request\n",
    "        start_index += 25\n",
    "        suffix = '/company/{}/filing-history?start_index={}'.format(comp_number, start_index)\n",
    "        url = url_template.format(suffix)\n",
    "        r, counter = request_api(url, apikey, counter)\n",
    "        \n",
    "    return comp_numbers, filing_dates, filing_descriptions, name_change, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform search from a list of company names and return 4 relational dataframes  \n",
    "\n",
    "def company_search(companies, url_template, apikey, counter):\n",
    "    \n",
    "    # Define company result lists\n",
    "    names = []\n",
    "    comp_numbers = []\n",
    "    addresses = []\n",
    "    kinds = []\n",
    "    creation_dates = []\n",
    "    dissolved_dates = []\n",
    "    active_officers = []\n",
    "    inactive_officers = []\n",
    "    resigned_officers = []\n",
    "    active_pscs = []\n",
    "    ceased_pscs = []\n",
    "\n",
    "    # Define officer result lists\n",
    "    officer_names = []\n",
    "    officer_roles = []\n",
    "    officer_types = []\n",
    "    officer_statuses = []\n",
    "    officer_countries = []\n",
    "    officer_addresses = []\n",
    "    officer_comp_numbers = []\n",
    "    \n",
    "    # Define PSC result lists\n",
    "    psc_types = []\n",
    "    psc_statuses = []\n",
    "    psc_countries = []\n",
    "    psc_comp_numbers = []\n",
    "    \n",
    "    # Define filing history result lists\n",
    "    filing_dates = []\n",
    "    filing_descriptions = []\n",
    "    filing_comp_numbers = []\n",
    "    filing_name_changes = []\n",
    "\n",
    "    # Iterate through company names\n",
    "    for company in tqdm_notebook(companies):\n",
    "        # Formulate search url and make request\n",
    "        suffix = '/search/companies?q=\"{}\"'.format(company.replace(' ', '+'))\n",
    "        url = url_template.format(suffix)\n",
    "        r, counter = request_api(url, apikey, counter)\n",
    "        \n",
    "        # Count number of items on page\n",
    "        r_items = min(r['items_per_page'], r['total_results'])\n",
    "\n",
    "        # Iterate through each company in search results \n",
    "        for i in range(r_items):\n",
    "        \n",
    "            # Check company name is a match\n",
    "            title = r['items'][i]['title'].strip().replace('  ', ' ')\n",
    "            if company.lower() in title.lower():\n",
    "                link = r['items'][i]['links']['self']    \n",
    "\n",
    "                # Fetch company information\n",
    "                name, comp_number, address, kind, creation_date, dissolved_date, counter =\\\n",
    "                fetch_comp_info(link, url_template, apikey, counter)\n",
    "\n",
    "                # Update company result lists for current company\n",
    "                names.append(name.lower())\n",
    "                comp_numbers.append(comp_number)\n",
    "                addresses.append(address)\n",
    "                kinds.append(kind)\n",
    "                creation_dates.append(creation_date)  \n",
    "                dissolved_dates.append(dissolved_date)\n",
    "\n",
    "                # Fetch officer information\n",
    "                num_active, num_inactive, num_resigned,\\\n",
    "                officer_name, officer_role, officer_type, officer_status, officer_country,\\\n",
    "                officer_address,officer_comp_number, counter =\\\n",
    "                fetch_officers(comp_number, url_template, apikey, counter)\n",
    "\n",
    "                # Update officer result lists for current company\n",
    "                active_officers.append(num_active)\n",
    "                inactive_officers.append(num_inactive)\n",
    "                resigned_officers.append(num_resigned)\n",
    "                officer_names.append(officer_name)\n",
    "                officer_roles.append(officer_role)\n",
    "                officer_types.append(officer_type)\n",
    "                officer_statuses.append(officer_status)\n",
    "                officer_countries.append(officer_country)\n",
    "                officer_addresses.append(officer_address)\n",
    "                officer_comp_numbers.append(officer_comp_number)\n",
    "\n",
    "                # Fetch PSC information\n",
    "                psc_active, psc_ceased, psc_type, psc_status, psc_country, psc_comp_number, counter =\\\n",
    "                fetch_psc(comp_number, url_template, apikey, counter)\n",
    "\n",
    "                # Update PSC result lists for current company\n",
    "                active_pscs.append(psc_active)\n",
    "                ceased_pscs.append(psc_ceased)\n",
    "                psc_types.append(psc_type)\n",
    "                psc_statuses.append(psc_status)\n",
    "                psc_countries.append(psc_country)\n",
    "                psc_comp_numbers.append(psc_comp_number)\n",
    "\n",
    "                # Fetch filing history\n",
    "                filing_comp_number, filing_date, filing_description, filing_name_change counter =\\\n",
    "                fetch_filings(comp_number, url_template, apikey, counter)\n",
    "\n",
    "                # Update filing result lists for current company\n",
    "                filing_comp_numbers.append(filing_comp_number)\n",
    "                filing_dates.append(filing_date)\n",
    "                filing_descriptions.append(filing_description)\n",
    "                filing_name_changes.append(filing_name_change)\n",
    "\n",
    "            # Stop searching once results no longer match company name\n",
    "            else:\n",
    "                break\n",
    "                    \n",
    "\n",
    "    # Flatten lists\n",
    "    officer_names = [i for sublist in officer_names for i in sublist]\n",
    "    officer_roles = [i for sublist in officer_roles for i in sublist]\n",
    "    officer_types = [i for sublist in officer_types for i in sublist]\n",
    "    officer_statuses = [i for sublist in officer_statuses for i in sublist]\n",
    "    officer_countries = [i for sublist in officer_countries for i in sublist]\n",
    "    officer_addresses = [i for sublist in officer_addresses for i in sublist]\n",
    "    officer_comp_numbers = [i for sublist in officer_comp_numbers for i in sublist]\n",
    "    psc_types = [i for sublist in psc_types for i in sublist]\n",
    "    psc_statuses = [i for sublist in psc_statuses for i in sublist]\n",
    "    psc_countries = [i for sublist in psc_countries for i in sublist]\n",
    "    psc_comp_numbers = [i for sublist in psc_comp_numbers for i in sublist]\n",
    "    filing_comp_numbers = [i for sublist in filing_comp_numbers for i in sublist]\n",
    "    filing_dates = [i for sublist in filing_dates for i in sublist]\n",
    "    filing_descriptions = [i for sublist in filing_descriptions for i in sublist]\n",
    "\n",
    "\n",
    "    # Create company results dataframe\n",
    "    company_results = pd.DataFrame({'name':names,\n",
    "                    'address':addresses,\n",
    "                    'kind':kinds,\n",
    "                    'company_number':comp_numbers,\n",
    "                    'creation_date':creation_dates,\n",
    "                    'dissolved_date':dissolved_dates,\n",
    "                    'active_officers':active_officers,\n",
    "                    'inactive_officers':inactive_officers,\n",
    "                    'resigned_officers': resigned_officers,\n",
    "                    'active_psc': active_pscs,\n",
    "                    'ceased_psc': ceased_pscs})\n",
    "\n",
    "    # Create officer results dataframe\n",
    "    officer_results = pd.DataFrame({'company_number': officer_comp_numbers,\n",
    "                                    'name': officer_names,\n",
    "                                    'type': officer_types,\n",
    "                                    'role': officer_roles,\n",
    "                                    'status': officer_statuses,\n",
    "                                    'country': officer_countries,\n",
    "                                    'address': officer_addresses})\n",
    "    \n",
    "    # Create PSC results dataframe\n",
    "    psc_results = pd.DataFrame({'company_number': psc_comp_numbers,\n",
    "                            'type': psc_types,\n",
    "                            'status': psc_statuses,\n",
    "                            'country': psc_countries})\n",
    "    \n",
    "    # Create filing results dataframe\n",
    "    filing_results = pd.DataFrame({'company_number': filing_comp_numbers,\n",
    "                                  'filing_date': filing_dates,\n",
    "                                  'description': filing_descriptions,\n",
    "                                  'name_change': filing_name_changes})\n",
    "\n",
    "\n",
    "    return company_results, officer_results, psc_results, filing_results, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform search from a list of company numbers and return 4 relational dataframes\n",
    "\n",
    "def company_number_search(company_nums, url_template, apikey, counter):\n",
    "    \n",
    "    # Define company result lists\n",
    "    names = []\n",
    "    comp_numbers = []\n",
    "    addresses = []\n",
    "    kinds = []\n",
    "    creation_dates = []\n",
    "    dissolved_dates = []\n",
    "    active_officers = []\n",
    "    inactive_officers = []\n",
    "    resigned_officers = []\n",
    "    active_pscs = []\n",
    "    ceased_pscs = []\n",
    "\n",
    "    # Define officer result lists\n",
    "    officer_names = []\n",
    "    officer_roles = []\n",
    "    officer_types = []\n",
    "    officer_statuses = []\n",
    "    officer_countries = []\n",
    "    officer_addresses = []\n",
    "    officer_comp_numbers = []\n",
    "    \n",
    "    # Define psc result lists\n",
    "    psc_types = []\n",
    "    psc_statuses = []\n",
    "    psc_countries = []\n",
    "    psc_comp_numbers = []\n",
    "    \n",
    "    # Define filing history result lists\n",
    "    filing_dates = []\n",
    "    filing_descriptions = []\n",
    "    filing_comp_numbers = []\n",
    "    filing_name_changes = []\n",
    "\n",
    "    # Iterate through company numbers\n",
    "    for company_num in tqdm_notebook(company_nums):\n",
    "        \n",
    "        # Formulate search url\n",
    "        link = '/company/{}'.format(company_num)\n",
    "\n",
    "        # Fetch company information\n",
    "        name, comp_number, address, kind, creation_date, dissolved_date, counter =\\\n",
    "        fetch_comp_info(link, url_template, apikey, counter)\n",
    "\n",
    "        # Update company result lists for current company\n",
    "        names.append(name.lower())\n",
    "        comp_numbers.append(comp_number)\n",
    "        addresses.append(address)\n",
    "        kinds.append(kind)\n",
    "        creation_dates.append(creation_date)   \n",
    "        dissolved_dates.append(dissolved_date)\n",
    "                    \n",
    "        # Fetch officer information\n",
    "        num_active, num_inactive, num_resigned, officer_name, officer_role, officer_type, officer_status,\\\n",
    "        officer_country, officer_address, officer_comp_number, counter =\\\n",
    "        fetch_officers(company_num, url_template, apikey, counter)\n",
    "\n",
    "        # Update officer result lists for current company\n",
    "        active_officers.append(num_active)\n",
    "        inactive_officers.append(num_inactive)\n",
    "        resigned_officers.append(num_resigned)\n",
    "        officer_names.append(officer_name)\n",
    "        officer_roles.append(officer_role)\n",
    "        officer_types.append(officer_type)\n",
    "        officer_statuses.append(officer_status)\n",
    "        officer_countries.append(officer_country)\n",
    "        officer_addresses.append(officer_address)\n",
    "        officer_comp_numbers.append(officer_comp_number)\n",
    "\n",
    "        # Fetch PSC information\n",
    "        psc_active, psc_ceased, psc_type, psc_status, psc_country, psc_comp_number, counter =\\\n",
    "        fetch_psc(company_num, url_template, apikey, counter)\n",
    "\n",
    "        # Update PSC result lists for current company\n",
    "        active_pscs.append(psc_active)\n",
    "        ceased_pscs.append(psc_ceased)\n",
    "        psc_types.append(psc_type)\n",
    "        psc_statuses.append(psc_status)\n",
    "        psc_countries.append(psc_country)\n",
    "        psc_comp_numbers.append(psc_comp_number)\n",
    "        \n",
    "        # Fetch filing history\n",
    "        filing_comp_number, filing_date, filing_description, filing_name_change, counter =\\\n",
    "        fetch_filings(company_num, url_template, apikey, counter)\n",
    "        \n",
    "        filing_comp_numbers.append(filing_comp_number)\n",
    "        filing_dates.append(filing_date)\n",
    "        filing_descriptions.append(filing_description)\n",
    "        filing_name_changes.append(filing_name_change)\n",
    "                    \n",
    "\n",
    "    # Flatten lists\n",
    "    officer_names = [i for sublist in officer_names for i in sublist]\n",
    "    officer_roles = [i for sublist in officer_roles for i in sublist]\n",
    "    officer_types = [i for sublist in officer_types for i in sublist]\n",
    "    officer_statuses = [i for sublist in officer_statuses for i in sublist]\n",
    "    officer_countries = [i for sublist in officer_countries for i in sublist]\n",
    "    officer_addresses = [i for sublist in officer_addresses for i in sublist]\n",
    "    officer_comp_numbers = [i for sublist in officer_comp_numbers for i in sublist]\n",
    "    psc_types = [i for sublist in psc_types for i in sublist]\n",
    "    psc_statuses = [i for sublist in psc_statuses for i in sublist]\n",
    "    psc_countries = [i for sublist in psc_countries for i in sublist]\n",
    "    psc_comp_numbers = [i for sublist in psc_comp_numbers for i in sublist]\n",
    "    filing_comp_numbers = [i for sublist in filing_comp_numbers for i in sublist]\n",
    "    filing_dates = [i for sublist in filing_dates for i in sublist]\n",
    "    filing_descriptions = [i for sublist in filing_descriptions for i in sublist]\n",
    "\n",
    "\n",
    "    # Create company results dataframe\n",
    "    company_results = pd.DataFrame({'name':names,\n",
    "                    'address':addresses,\n",
    "                    'kind':kinds,\n",
    "                    'company_number':comp_numbers,\n",
    "                    'creation_date':creation_dates,\n",
    "                    'dissolved_date':dissolved_dates,\n",
    "                    'active_officers':active_officers,\n",
    "                    'inactive_officers':inactive_officers,\n",
    "                    'resigned_officers': resigned_officers,\n",
    "                    'active_psc': active_pscs,\n",
    "                    'ceased_psc': ceased_pscs})\n",
    "\n",
    "    # Create officer results dataframe\n",
    "    officer_results = pd.DataFrame({'company_number': officer_comp_numbers,\n",
    "                                    'name': officer_names,\n",
    "                                    'role': officer_roles,\n",
    "                                    'type': officer_types,\n",
    "                                    'status': officer_statuses,\n",
    "                                    'country': officer_countries,\n",
    "                                    'address': officer_addresses})\n",
    "    \n",
    "    # Create PSC results dataframe\n",
    "    psc_results = pd.DataFrame({'company_number': psc_comp_numbers,\n",
    "                            'status': psc_statuses,\n",
    "                            'type': psc_types,\n",
    "                            'country': psc_countries})\n",
    "    \n",
    "    # Create filing results dataframe\n",
    "    filing_results = pd.DataFrame({'company_number': filing_comp_numbers,\n",
    "                                  'filing_date': filing_dates,\n",
    "                                  'description': filing_descriptions,\n",
    "                                  'name_change': filing_name_changes})\n",
    "\n",
    "\n",
    "    return company_results, officer_results, psc_results, filing_results, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect all companies linked to an officer\n",
    "\n",
    "def fetch_company_from_officer(officer_link, url_template, apikey, counter):\n",
    "    # Instantiate results list\n",
    "    comp_numbers = []\n",
    "    \n",
    "    # Formulate url and make request\n",
    "    url = url_template.format(officer_link)\n",
    "    r, counter = request_api(url, apikey, counter)\n",
    "    \n",
    "    # Iterate through all companies for officer\n",
    "    for i in range(len(r['items'])):\n",
    "        \n",
    "        # Extract company number from link\n",
    "        comp_number = r['items'][i]['links']['company'].split('/')[-1]\n",
    "        # Update results list with current company number\n",
    "        comp_numbers.append(comp_number)\n",
    "        \n",
    "    return comp_numbers, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform search for companies from list of officers \n",
    "\n",
    "def officer_search(officer_list, url_template, apikey, counter):\n",
    "\n",
    "    # Define company result lists\n",
    "    names = []\n",
    "    comp_numbers = []\n",
    "    addresses = []\n",
    "    kinds = []\n",
    "    creation_dates = []\n",
    "    dissolved_dates = []\n",
    "    active_officers = []\n",
    "    inactive_officers = []\n",
    "    resigned_officers = []\n",
    "    active_pscs = []\n",
    "    ceased_pscs = []\n",
    "\n",
    "    # Define officer result lists\n",
    "    officer_names = []\n",
    "    officer_roles = []\n",
    "    officer_types = []\n",
    "    officer_statuses = []\n",
    "    officer_countries = []\n",
    "    officer_addresses = []\n",
    "    officer_comp_numbers = []\n",
    "    \n",
    "    # Define psc result lists\n",
    "    psc_types = []\n",
    "    psc_statuses = []\n",
    "    psc_countries = []\n",
    "    psc_comp_numbers = []\n",
    "    \n",
    "    # Define filing history result lists\n",
    "    filing_dates = []\n",
    "    filing_descriptions = []\n",
    "    filing_comp_numbers = []\n",
    "    filing_name_changes = []\n",
    "\n",
    "    # Iterate through officers\n",
    "    for officer in tqdm_notebook(officer_list):\n",
    "        \n",
    "        # Initialise list of company numbers collected from officer\n",
    "        comp_list = []\n",
    "        \n",
    "        # Intialise page count\n",
    "        start_index = 0\n",
    "        \n",
    "        # Formulate url and make request\n",
    "        suffix = '/search/officers?q=\"{}\"&start_index={}'.format(officer.replace(' ', '+'), start_index)\n",
    "        url = url_template.format(suffix)\n",
    "        r, counter = request_api(url, apikey, counter)\n",
    "        \n",
    "        # Find officer title, remove double spaces\n",
    "        title = r['items'][0]['title'].strip().replace('  ', ' ') \n",
    "        \n",
    "        # Stop searching when result no longer matches searched name\n",
    "        while officer.lower() in title.lower():\n",
    "            \n",
    "            # Check number of results on page\n",
    "            r_items = min(r['items_per_page'], r['total_results'])\n",
    "            \n",
    "            # Iterate through results\n",
    "            for i in tqdm_notebook(range(r_items)):\n",
    "                \n",
    "                # Find title of current search result\n",
    "                title = r['items'][i]['title'].strip().replace('  ', ' ')\n",
    "                    \n",
    "                # Collect list of company numbers\n",
    "                comp_nums, counter = fetch_company_from_officer(link, url_template, apikey, counter)\n",
    "                comp_list.append(comp_nums)\n",
    "            \n",
    "            # Check if final page\n",
    "            if r['page_number']*r['items_per_page'] < r['total_results']:\n",
    "                \n",
    "                # Increment to next page of results\n",
    "                start_index += 20\n",
    "                \n",
    "                # Formulate url and make request\n",
    "                suffix = '/search/officers?q=\"{}\"&start_index={}'.format(officer.replace(' ', '+'), start_index)\n",
    "                url = url_template.format(suffix)\n",
    "                r, counter = request_api(url, apikey, counter)\n",
    "                \n",
    "                # Find title of first result\n",
    "                title = r['items'][0]['title'].strip().replace('  ', ' ')\n",
    "            \n",
    "            # Stop once final page reached\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Flatten list\n",
    "        comp_list = [i for sublist in comp_list for i in sublist]\n",
    "        \n",
    "        # Iterate through result list of company numbers\n",
    "        for comp_num in tqdm_notebook(comp_list):\n",
    "\n",
    "            # Fetch company information\n",
    "            link = '/company/'+ comp_num\n",
    "            name, comp_number, address, kind, creation_date, dissolved_date, counter =\\\n",
    "            fetch_comp_info(link, url_template, apikey, counter)\n",
    "\n",
    "            # Update company result lists for current company\n",
    "            names.append(name.lower())\n",
    "            comp_numbers.append(comp_number)\n",
    "            addresses.append(address)\n",
    "            kinds.append(kind)\n",
    "            creation_dates.append(creation_date)  \n",
    "            dissolved_dates.append(dissolved_date)\n",
    "\n",
    "            # Fetch officer information\n",
    "            num_active, num_inactive, num_resigned, officer_name, officer_role, officer_type, officer_status,\\\n",
    "            officer_country, officer_address, officer_comp_number, counter =\\\n",
    "            fetch_officers(comp_number, url_template, apikey, counter)\n",
    "\n",
    "            # Update officer result lists for current company\n",
    "            active_officers.append(num_active)\n",
    "            inactive_officers.append(num_inactive)\n",
    "            resigned_officers.append(num_resigned)\n",
    "            officer_names.append(officer_name)\n",
    "            officer_roles.append(officer_role)\n",
    "            officer_types.append(officer_type)\n",
    "            officer_statuses.append(officer_status)\n",
    "            officer_countries.append(officer_country)\n",
    "            officer_addresses.append(officer_address)\n",
    "            officer_comp_numbers.append(officer_comp_number)\n",
    "\n",
    "            # Fetch PSC information\n",
    "            psc_active, psc_ceased, psc_type, psc_status, psc_country, psc_comp_number, counter =\\\n",
    "            fetch_psc(comp_number, url_template, apikey, counter)\n",
    "\n",
    "            # Update PSC result lists for current company\n",
    "            active_pscs.append(psc_active)\n",
    "            ceased_pscs.append(psc_ceased)\n",
    "            psc_types.append(psc_type)\n",
    "            psc_statuses.append(psc_status)\n",
    "            psc_countries.append(psc_country)\n",
    "            psc_comp_numbers.append(psc_comp_number)\n",
    "            \n",
    "            # Fetch filing history\n",
    "            filing_comp_number, filing_date, filing_description, filing_name_change, counter =\\\n",
    "            fetch_filings(comp_number, url_template, apikey, counter)\n",
    "\n",
    "            # Update filing result lists for current company\n",
    "            filing_comp_numbers.append(filing_comp_number)\n",
    "            filing_dates.append(filing_date)\n",
    "            filing_descriptions.append(filing_description)\n",
    "            filing_name_changes.append(filing_name_change)\n",
    "                    \n",
    "\n",
    "    # Flatten lists\n",
    "    officer_names = [i for sublist in officer_names for i in sublist]\n",
    "    officer_roles = [i for sublist in officer_roles for i in sublist]\n",
    "    officer_types = [i for sublist in officer_types for i in sublist]\n",
    "    officer_statuses = [i for sublist in officer_statuses for i in sublist]\n",
    "    officer_countries = [i for sublist in officer_countries for i in sublist]\n",
    "    officer_addresses = [i for sublist in officer_addresses for i in sublist]\n",
    "    officer_comp_numbers = [i for sublist in officer_comp_numbers for i in sublist]\n",
    "    psc_types = [i for sublist in psc_types for i in sublist]\n",
    "    psc_statuses = [i for sublist in psc_statuses for i in sublist]\n",
    "    psc_countries = [i for sublist in psc_countries for i in sublist]\n",
    "    psc_comp_numbers = [i for sublist in psc_comp_numbers for i in sublist]\n",
    "    filing_comp_numbers = [i for sublist in filing_comp_numbers for i in sublist]\n",
    "    filing_dates = [i for sublist in filing_dates for i in sublist]\n",
    "    filing_descriptions = [i for sublist in filing_descriptions for i in sublist]\n",
    "\n",
    "\n",
    "    # Create company results dataframe\n",
    "    company_results = pd.DataFrame({'name':names,\n",
    "                    'address':addresses,\n",
    "                    'kind':kinds,\n",
    "                    'company_number':comp_numbers,\n",
    "                    'creation_date':creation_dates,\n",
    "                    'dissolved_date':dissolved_dates,\n",
    "                    'active_officers':active_officers,\n",
    "                    'inactive_officers':inactive_officers,\n",
    "                    'resigned_officers': resigned_officers,\n",
    "                    'active_psc': active_pscs,\n",
    "                    'ceased_psc': ceased_pscs})\n",
    "\n",
    "    # Create officer results dataframe\n",
    "    officer_results = pd.DataFrame({'company_number': officer_comp_numbers,\n",
    "                                    'name': officer_names,\n",
    "                                    'role': officer_roles,\n",
    "                                    'type': officer_types,\n",
    "                                    'status': officer_statuses,\n",
    "                                    'country': officer_countries,\n",
    "                                    'address': officer_addresses})\n",
    "    \n",
    "    # Create PSC results dataframe\n",
    "    psc_results = pd.DataFrame({'company_number': psc_comp_numbers,\n",
    "                            'type': psc_types,\n",
    "                            'status': psc_statuses,\n",
    "                            'country': psc_countries})\n",
    "    \n",
    "    # Create filing results dataframe\n",
    "    filing_results = pd.DataFrame({'company_number': filing_comp_numbers,\n",
    "                                   'filing_date': filing_dates,\n",
    "                                   'description': filing_descriptions,\n",
    "                                   'name_change':filing_name_changes})\n",
    "\n",
    "\n",
    "    return company_results, officer_results, psc_results, filing_results, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Read in data from research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import list of names for flagged companies\n",
    "flagged_comp_list = aml_private.flagged_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import list of names for flagged officers\n",
    "flagged_list_officers = aml_private.flagged_officers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in laundromat file\n",
    "ml_df = pd.read_excel('Resource_files/laundromat-companies.xlsx')\n",
    "\n",
    "# Remove entries with no name\n",
    "ml_df.dropna(subset=['name'], inplace=True)\n",
    "\n",
    "# Select only companies linked to at least 10 transactions and where both sending and receiving\n",
    "ml_df = ml_df[(ml_df.tx_count>10)&(ml_df.amount_usd_in>0)&(ml_df.amount_usd_out>0)]\n",
    "\n",
    "# Select only company names \n",
    "ml_list = ml_df.reset_index(drop=True).name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Collect positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22767dd82de645dbaf2a748b9c627d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=90), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Scrape companies from flagged companies list\n",
    "flagged_comps, flagged_officers, flagged_psc, flagged_filings, request_count =\\\n",
    "company_search(flagged_comp_list, url_template, apikey, request_count)\n",
    "\n",
    "# Scrape companies from flagged officers list\n",
    "flagged_comps_2, flagged_officers_2, flagged_psc_2, flagged_filings_2, request_count =\\\n",
    "officer_search(flagged_list_officers, url_template, apikey, request_count)\n",
    "\n",
    "# Combine flagged results\n",
    "flagged_comps = pd.concat([flagged_comps, flagged_comps_2]).reset_index(drop=True)\n",
    "flagged_offs = pd.concat([flagged_offs, flagged_offs_2]).reset_index(drop=True)\n",
    "flagged_psc = pd.concat([flagged_psc, flagged_psc_2]).reset_index(drop=True)\n",
    "flagges_filings = pd.concat([flagged_filings, flagged_filings_2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd028d82b4184bf8ae579f413b6935cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=240), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 1, 3, 45, 383903)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scrape fraudulent companies from laundromat file\n",
    "laund_comps, laund_officers, laund_psc, laund_filings, request_count =\\\n",
    "company_search(ml_list, url_template, apikey, request_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbe25b3a5da444aac60c7ed730fdf2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1433), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 9, 7, 12, 221055)}\n",
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 9, 12, 13, 232351)}\n",
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 9, 17, 14, 241090)}\n",
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 9, 22, 15, 204973)}\n",
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 9, 27, 16, 215599)}\n",
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 9, 32, 17, 225899)}\n",
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 9, 37, 18, 210785)}\n",
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 9, 42, 19, 221900)}\n",
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 9, 47, 20, 230173)}\n",
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 9, 52, 21, 217042)}\n",
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 9, 57, 22, 224800)}\n",
      "{'count': 1, 'reset_time': datetime.datetime(2020, 2, 26, 10, 2, 23, 232288)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in company factory files\n",
    "cornwall_df = pd.read_excel('Resource_files/Cornwall_Buildings.xlsx')\n",
    "cornwall_list = cornwall_df[['Registration number']]\n",
    "cornwall_list = cornwall_list.reset_index(drop=True)\n",
    "\n",
    "darkes_df = pd.read_excel('Resource_files/Darkes_Lane.xlsx')\n",
    "darkes_list = darkes_df[['Registration number']]\n",
    "darkes_list = darkes_list.reset_index(drop=True)\n",
    "\n",
    "churchill_df = pd.read_excel('Resource_files/Churchill_Court.xlsx')\n",
    "churchill_list = churchill_df[['Registration number']]\n",
    "churchill_list = churchill_list.reset_index(drop=True)\n",
    "\n",
    "coburg_df = pd.read_excel('Resource_files/Coburg_Road.xlsx')\n",
    "coburg_list = coburg_df[['Registration number']]\n",
    "coburg_list = coburg_list.reset_index(drop=True)\n",
    "\n",
    "# Create list from all company numbers\n",
    "company_factories = list(pd.concat([cornwall_list, darkes_list, coburg_list])['Registration number'])\n",
    "\n",
    "# Scrape companies from company factory files\n",
    "factory_comps, factory_officers, factory_psc, factory_filings, request_count =\\\n",
    "company_number_search(company_factories, url_template, apikey, request_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine all positive cases\n",
    "ml_comps = pd.concat([flagged_comps,laund_comps,factory_comps], sort=False).reset_index(drop=True)\n",
    "\n",
    "ml_officers = pd.concat([flagged_officers,laund_officers,factory_officers], sort=False).reset_index(drop=True)\n",
    "\n",
    "ml_psc = pd.concat([flagged_psc,laund_psc,factory_psc], sort=False).reset_index(drop=True)\n",
    "\n",
    "ml_filings = pd.concat([flagged_filings,laund_filings,factory_filings], sort=False).reset_index(drop=True)\n",
    "\n",
    "# Remove duplicates\n",
    "ml_comps.drop_duplicates(inplace=True)\n",
    "ml_officers.drop_duplicates(inplace=True)\n",
    "ml_psc.drop_duplicates(inplace=True)\n",
    "ml_filings.drop_duplicates(inplace=True)\n",
    "\n",
    "# Label data as positive for suspicious\n",
    "ml_comps['ml'] = 1\n",
    "\n",
    "# Save data to csv\n",
    "ml_comps.to_csv('ml_companies.csv', index=False)\n",
    "ml_officers.to_csv('ml_officers.csv', index=False)\n",
    "ml_psc.to_csv('ml_psc.csv', index=False)\n",
    "ml_filings.to_csv('ml_filings.csv', index=False)\n",
    "\n",
    "# Count total results\n",
    "ml_rows = ml_comps.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Collect negative samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in files for entire CH database\n",
    "non_ml_df_1 = pd.read_csv('Resource_files/BasicCompanyData-2020-02-01-part1_6.csv')\n",
    "non_ml_df_2 = pd.read_csv('Resource_files/BasicCompanyData-2020-02-01-part2_6.csv')\n",
    "non_ml_df_3 = pd.read_csv('Resource_files/BasicCompanyData-2020-02-01-part3_6.csv')\n",
    "non_ml_df_4 = pd.read_csv('Resource_files/BasicCompanyData-2020-02-01-part4_6.csv')\n",
    "non_ml_df_5 = pd.read_csv('Resource_files/BasicCompanyData-2020-02-01-part5_6.csv')\n",
    "non_ml_df_6 = pd.read_csv('Resource_files/BasicCompanyData-2020-02-01-part6_6.csv')\n",
    "\n",
    "# Combine into single dataframe\n",
    "non_ml_df = pd.concat([non_ml_df_1,\\\n",
    "                       non_ml_df_2,\\\n",
    "                       non_ml_df_3,\\\n",
    "                       non_ml_df_4,\\\n",
    "                       non_ml_df_5,\\\n",
    "                       non_ml_df_6], sort=False).reset_index(drop=True)\n",
    "\n",
    "# Remove companies without registered address\n",
    "non_ml_df = non_ml_df[non_ml_df['RegAddress.AddressLine1'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Randomly sample n+10 company numbers as redundancy in case of overlap with ml companies\n",
    "sample_comp_nums = list(non_ml_df.sample(n=(ml_rows+10)).loc[:, ' CompanyNumber'])\n",
    "\n",
    "# Check if any of random sample appear in positive cases and remove if so \n",
    "overlaps = ml_comps[ml_comps['company_number'].isin(sample_comp_nums)].company_number\n",
    "\n",
    "# Remove by company number if overlap in positive cases\n",
    "if len(overlaps) > 0:\n",
    "    for overlap in overlaps:\n",
    "        sample_comp_nums.remove(overlap)\n",
    "    \n",
    "# Remove excess companies to match positive sample size\n",
    "excess = 10-len(overlaps)\n",
    "    for i in range(excess):\n",
    "        sample_comp_nums.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scrape non-fraudulent companies\n",
    "non_ml_comps, non_ml_officers, non_ml_psc, non_ml_filings, request_count =\\\n",
    "company_number_search(sample_comp_nums, url_template, apikey, request_count)\n",
    "\n",
    "# Label data as negative, ie not suspicious\n",
    "non_ml_comps['ml'] = 0\n",
    "\n",
    "# Save data to csv\n",
    "non_ml_comps.to_csv('non_ml_companies.csv', index=False)\n",
    "non_ml_officers.to_csv('non_ml_officers.csv', index=False)\n",
    "non_ml_psc.to_csv('non_ml_psc.csv', index=False)\n",
    "non_ml_filings.to_csv('non_ml_filings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save data to csv\n",
    "non_ml_comps.to_csv('non_ml_companies.csv', index=False)\n",
    "non_ml_officers.to_csv('non_ml_officers.csv', index=False)\n",
    "non_ml_psc.to_csv('non_ml_psc.csv', index=False)\n",
    "non_ml_filings.to_csv('non_ml_filings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "225px",
    "left": "604px",
    "right": "20px",
    "top": "128px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
